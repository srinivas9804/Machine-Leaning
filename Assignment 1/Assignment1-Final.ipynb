{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** SIVAKUMAR Srinivas\n",
    "\n",
    "**EID:** ssivakuma2\n",
    "\n",
    "**Kaggle Team Name:** Shaata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4487 - Assignment 1 - YouTube Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission\n",
    "In this file, put the code that generates your final Kaggle submission. It will be used to verify that your Kaggle submission is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "random.seed(100)\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write a CSV file for Kaggle submission\n",
    "def write_csv_kaggle_sub(fname, Y):\n",
    "    # fname = file name\n",
    "    # Y is a list/array with class entries\n",
    "    with open(fname, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        # header\n",
    "        tmp = ['Id', 'Prediction']            \n",
    "        writer.writerow(tmp)\n",
    "    \n",
    "        # add ID numbers for each Y, and usage if necessary\n",
    "        for (i,y) in enumerate(Y):\n",
    "            tmp2 = [(i+1), y]\n",
    "            writer.writerow(tmp2)\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "956\n"
     ]
    }
   ],
   "source": [
    "# load the data with pandas\n",
    "traindf = pd.read_csv('youtube_spam_train.csv')\n",
    "testdf  = pd.read_csv('youtube_spam_test.csv')\n",
    "\n",
    "# extract data into a dictionary\n",
    "\n",
    "train = {}                                 ### training data\n",
    "train['txt']   = list(traindf['TEXT'])     # comment text\n",
    "train['class'] = list(traindf['CLASS'])    # class (0=not spam; 1=spam)\n",
    "train['author'] = list(traindf['AUTHOR'])  # author name\n",
    "train['video']  = list(traindf['VIDEO'])   # video ID\n",
    "train['date']   = list(traindf['DATE'])    # date of the comment\n",
    "\n",
    "test = {}                                  ### testing data\n",
    "test['txt']   = list(testdf['TEXT'])\n",
    "test['author'] = list(testdf['AUTHOR'])\n",
    "test['video']  = list(testdf['VIDEO'])\n",
    "test['date']    = list(testdf['DATE'])\n",
    "\n",
    "print(len(train['txt']))\n",
    "print(len(test['txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/elft18/ssivakuma2/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def replace_url_in_doc(doc):\n",
    "    doc = re.sub('(/^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$/)', 'URL', doc)\n",
    "    return re.sub('(/^(www\\.[^\\s]+)|(https?://[^\\s]+))','URL', doc) \n",
    "\n",
    "for i in range(len(traindf['TEXT'])):\n",
    "    text = traindf['TEXT'][i]\n",
    "    text = replace_url_in_doc(text)\n",
    "    words = text.split()\n",
    "    text = ''\n",
    "    for j in words:\n",
    "        if(text == ''):\n",
    "            text += wordnet_lemmatizer.lemmatize(j.lower(),pos='v')\n",
    "        else:\n",
    "            text = text +' '+wordnet_lemmatizer.lemmatize(j.lower(),pos='v')\n",
    "    traindf['TEXT'][i] = text \n",
    "    \n",
    "for i in range(len(testdf['TEXT'])):\n",
    "    text = testdf['TEXT'][i]\n",
    "    text = replace_url_in_doc(text)\n",
    "    words = text.split()\n",
    "    text = ''\n",
    "    for j in words:\n",
    "        if(text == ''):\n",
    "            text += wordnet_lemmatizer.lemmatize(j.lower(),pos='v')\n",
    "        else:\n",
    "            text = text +' '+wordnet_lemmatizer.lemmatize(j.lower(),pos='v')\n",
    "    testdf['TEXT'][i] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "import string\n",
    "import numpy as np\n",
    "class ColumnExtractor(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        Xcols = X[self.cols]\n",
    "        return Xcols\n",
    "    \n",
    "class ConvertToList(BaseEstimator,TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        Xlist = X.values.tolist()\n",
    "        return Xlist\n",
    "\n",
    "class LengthTransformer(BaseEstimator,TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        #X is a list\n",
    "        array = []\n",
    "        for t in X:\n",
    "            array.append(len(t))\n",
    "        return np.array(array).reshape(-1,1)\n",
    "\n",
    "class AuthorTransformer(BaseEstimator,TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        #X is a pandas df with two fields, 'TEXT' and 'AUTHOR'\n",
    "        array = []\n",
    "        AuthorList = list(X['AUTHOR'])\n",
    "        CommentList = list(X['TEXT'])\n",
    "        for i in range(len(AuthorList)):\n",
    "            flag = True\n",
    "            words = AuthorList[i].split()\n",
    "            for word in words:\n",
    "                if word in CommentList[i]:\n",
    "                    flag = False\n",
    "                    array.append(True)\n",
    "                    break\n",
    "            if flag:\n",
    "                array.append(False)\n",
    "        return np.array(array).reshape(-1,1)\n",
    "class PunctuationTransformer(BaseEstimator,TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "        array = []\n",
    "        for t in X:\n",
    "            array.append(count(t, string.punctuation))\n",
    "        return np.array(array).reshape(-1,1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1200 candidates, totalling 6000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 668 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1368 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 2268 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3368 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 4668 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6000 out of 6000 | elapsed:   38.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('text', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnExtractor(cols='TEXT')), ('convert', ConvertToList()), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class__C': array([1.00000e-04, 2.63665e-04, 6.95193e-04, 1.83298e-03, 4.83293e-03,\n",
       "       1.27427e-02, 3.35982e-02, 8.85867e-02, 2.33572e-01, 6.15848e-01,\n",
       "       1.62378e+00, 4.28133e+00, 1.12884e+01, 2.97635e+01, 7.84760e+01,\n",
       "       2.06914e+02, 5.45559e+02, 1.43845e+03, 3.79269e+03, 1...35, 4357, 4678, 5000]), 'features__text__vectorizer__ngram_range': ((1, 4), (1, 2), (1, 3), (1, 1))},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "logregPipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('extract',ColumnExtractor('TEXT')),\n",
    "            ('convert',ConvertToList()),\n",
    "            ('vectorizer', feature_extraction.text.CountVectorizer(stop_words='english', max_features=1400))\n",
    "        ])),\n",
    "        ('essay_length', Pipeline([\n",
    "            ('extract',ColumnExtractor('TEXT')),\n",
    "            ('convert',ConvertToList()),\n",
    "            ('length',LengthTransformer()),\n",
    "            ('scaler',StandardScaler())\n",
    "        ])),\n",
    "        ('author_name',  Pipeline([\n",
    "            ('extract',ColumnExtractor(['TEXT','AUTHOR'])),\n",
    "            ('author',AuthorTransformer())\n",
    "        ])),\n",
    "        ('punctuations',  Pipeline([\n",
    "            ('extract',ColumnExtractor(['TEXT'])),\n",
    "            ('convert',ConvertToList()),\n",
    "            ('punctuations',PunctuationTransformer()),\n",
    "            ('scaler',StandardScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('class',linear_model.LogisticRegression(C=100))\n",
    "])\n",
    "paramgrid = {'class__C': logspace(-4,4,20),'features__text__vectorizer__max_features':linspace(500,5000,15,dtype = int),\\\n",
    "        'features__text__vectorizer__ngram_range':((1, 4),(1,2),(1,3),(1,1))}\n",
    "logreg = model_selection.GridSearchCV(logregPipeline, paramgrid, cv=5,\n",
    "                                      n_jobs = -1, verbose=1)\n",
    "logreg.fit(traindf, traindf['CLASS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.944\n",
      "{'class__C': 11.288378916846883, 'features__text__vectorizer__max_features': 500, 'features__text__vectorizer__ngram_range': (1, 4)}\n"
     ]
    }
   ],
   "source": [
    "print(logreg.best_score_)\n",
    "print(logreg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predY = logreg.predict(testdf)\n",
    "write_csv_kaggle_sub(\"final_submission.csv\", predY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
